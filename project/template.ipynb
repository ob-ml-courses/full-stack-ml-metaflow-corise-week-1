{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Full Stack Machine Learning's Week 1 Project!\n",
    "\n",
    "Welcome to our project on sentiment analysis using a [Women's Ecommerce Clothing Reviews Dataset from Kaggle](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews)! As a Data Scientist, you'll often be tasked with building predictive models that help businesses understand their customers' needs and preferences. In this project, our main goal is to understand the importance of creating a baseline model as a starting point for further improvements.\n",
    "\n",
    "To achieve this goal, we'll be using a dataset containing reviews written by customers about women's clothing sold online. The dataset offers a great environment to parse out the text through its multiple dimensions, and because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with \"retailer\".\n",
    "\n",
    "As part of the project, we'll start by exploring the dataset, performing some basic cleaning, and establishing a baseline model using a simple algorithm. We'll then discuss the importance of this baseline model and how it can be used as a reference point for future improvements.\n",
    "\n",
    "Optionally, we'll also be creating a simple model that performs better than the baseline model. This will give us a better understanding of how machine learning models work and the benefits of using more complex algorithms.\n",
    "\n",
    "We're excited to embark on this project together and look forward to exploring the world of sentiment analysis with you!\n",
    "\n",
    "### Using GitHub\n",
    "To complete the assignment:\n",
    "1. Fill in the TODO sections of this notebook.\n",
    "2. Push the results to your `full-stack-ml-metaflow-corise-week-1` repository.\n",
    "3. Create a link to the repository in Corise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is a critical step in the data science pipeline as it allows us to gain insights and identify patterns within the data. In this section, we will be performing EDA on the Women's Clothing E-Commerce dataset, which contains reviews written by customers. Through this process, we will be looking out for trends, anomalies, and outliers that can help us better understand the data and inform our decision-making in subsequent stages of the project. By performing EDA, we will be able to identify potential issues with the dataset and make necessary corrections before proceeding to the model building phase.\n",
    "\n",
    "Suggestion: Spend 1-2 hours on this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies\n",
    "You can change these if you wish! \n",
    "These packages are already installed in the `full-stack-metaflow-corise` environment. \n",
    "If you are feeling adventurous, you can install other packages you want in the conda environment too, or even make your own environment from scratch and include with your submission! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure plots\n",
    "This part is optional styling your plots and cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YELLOW = '#FFBC00'\n",
    "GREEN = '#37795D'\n",
    "PURPLE = '#5460C0'\n",
    "BACKGROUND = '#F4EBE6'\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    'axes.spines.right': False, 'axes.spines.top': False,\n",
    "    'axes.facecolor':BACKGROUND, 'figure.facecolor': BACKGROUND, \n",
    "    'figure.figsize':(8, 8)\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style='ticks', rc=custom_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset, ensure to use index_col=0 when reading the CSV file. \n",
    "# Hints\n",
    "    # Look in the ../../data directory of this worksapce. \n",
    "    # If you use pandas think about the index_col arg ðŸ§ \n",
    "df = ...\n",
    "\n",
    "# light data cleaning\n",
    "df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "df['review_text'] = df['review_text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distribution of [1, 5] ratings\n",
    "\n",
    "We will be using the `rating` to create a label on this dataset. We can see that the mean rating is above 4, pretty happy customers!\n",
    "\n",
    "Let's try to visualise the distrbution of the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Derive the rating_distribution and plot it\n",
    "rating_distribution = ... \n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "# You can swap the color used with the defined constants at the top of the notebook\n",
    "ax.bar(x=rating_distribution.index, height=rating_distribution.values, color=GREEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_function(row):\n",
    "    \"\"\"\n",
    "    A function to derive labels from the user's review data.\n",
    "    This could use many variables, or just one. \n",
    "    In supervised learning scenarios, this is a very important part of determining what the machine learns!\n",
    "   \n",
    "    A subset of variables in the e-commerce fashion review dataset to consider for labels you could use in ML tasks include:\n",
    "        # rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "        # recommended_ind: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "        # positive_feedback_count: Positive Integer documenting the number of other customers who found this review positive.\n",
    "\n",
    "    In this case, we are doing sentiment analysis. \n",
    "    To keep things simple, we use the rating only, and return a binary positive or negative sentiment score based on an arbitrarty cutoff. \n",
    "    \"\"\"\n",
    "    # TODO: Add your logic for the labelling function here\n",
    "    # It is up to you on what value to choose as the cut off point for the postive class\n",
    "    # A good value to start would be 4\n",
    "    # This function should return either a 0 or 1 depending on the rating of a particular row\n",
    "    return\n",
    "\n",
    "# final features and labels\n",
    "_has_review_df = df[df['review_text'] != 'nan']\n",
    "reviews = _has_review_df['review_text']\n",
    "labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "has_review_df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "del _has_review_df\n",
    "\n",
    "# a few checks\n",
    "assert labels.shape == reviews.shape, \"Labels and reviews should be equal shape vectors!\"\n",
    "assert not sum([1 if r == 'nan' else 0 for r in reviews]) > 0, \"There are `nan` values in the feature set!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of points does your algorithm label with positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_positive_sentiment = labels.sum() / labels.shape[0]\n",
    "print(f\"{round(100*pct_positive_sentiment,3)}% of the labels have positive sentiment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us try to visualise the data that we just labeled depending on the rating. \n",
    "\n",
    "In a real world project, iterating at this point is crucial. You need to look through the way your data is labeled, and ensure it is aligned with your intuitive understanding and objectives of the algorithm. There are also automated tools to aid your label cleaning operations, such as [Cleanlab](https://github.com/cleanlab/cleanlab).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "positive_color = 'green'\n",
    "negative_color = 'red'\n",
    "N = 10\n",
    "\n",
    "# fetch subset of data\n",
    "idxs = np.random.choice(reviews.index, 10, replace=False)\n",
    "_labels_subset = labels[idxs]\n",
    "_reviews_subset = reviews[idxs]\n",
    "\n",
    "# print each sample and color the text by sentiment\n",
    "for label, review in zip(_labels_subset, _reviews_subset):\n",
    "    color = negative_color if label == 0 else positive_color\n",
    "    print(colored(review, color), end='\\n\\n')\n",
    "\n",
    "# in a real world project, iterating at this point is crucial.\n",
    "# you need to look through the way your data is labeled, and ensure it is aligned with your intuitive understanding and objectives of the algorithm.\n",
    "# there are also automated tools to aid your label cleaning operations, such as: https://github.com/cleanlab/cleanlab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think about the text and their corresponding labels? \n",
    "- Do you think the labels fit the text? \n",
    "- If not what do you think we can do to fix it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Stop Words\n",
    "In this part we will be filtering the stop words from the reviews. We remove stopwords in NLP datasets because there are words that do not carry much meaning on their own, and their presence can add noise to the analysis. These words are common and frequently occurring words such as \"a\", \"an\", \"the\", \"of\", and \"and\". \n",
    "\n",
    "Removing stopwords can improve the accuracy and efficiency of natural language processing tasks, such as sentiment analysis or topic modeling, by reducing the dimensionality of the data and increasing the signal-to-noise ratio. By removing these uninformative words, the resulting dataset may contain more meaningful information that can be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "non_stopwords = []\n",
    "for review in reviews: \n",
    "    for word in review.split():\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "        if word == '':\n",
    "            continue\n",
    "        if not word.lower() in stopwords:\n",
    "            non_stopwords.append(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the K most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 25\n",
    "words, counts = zip(*Counter(non_stopwords).most_common(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(12,5))\n",
    "plt.xticks(rotation = 55)\n",
    "ax.bar(x=words, height=counts, color=GREEN);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the other features in the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=has_review_df, hue='label', corner=True, palette=sns_palette);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Scoping Out a Machine Learning Project\n",
    "\n",
    "Welcome to the Project Manager task for the sentiment analysis classifier project. As a Data Scientist, you know that a successful project requires not only technical skills but also effective project management. In this task, you will take on the role of a Data Scientist tasked with leading the development of a sentiment analysis classifier. You will be responsible for planning and executing the project, ensuring that it aligns with business goals, stays within scope, and delivers value to stakeholders.\n",
    "\n",
    "To do this, you will create a one-page document that outlines the business value of the project, its scope, how to measure and monitor success, and when to quit. This task is designed to challenge you to think holistically about the project, and to consider not only the technical details but also the broader context in which the project is situated. Good luck!\n",
    "\n",
    "Suggestion: Spend 1-2 hours on this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the section below with your answers!\n",
    "\n",
    "### 1. The business value\n",
    "\n",
    "### 2. The scope\n",
    "\n",
    "### 3. How to measure success\n",
    "\n",
    "### 4. How to monitor success"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job completing Task 2! \n",
    "\n",
    "Why do you think it is important to create a one-page document prior to beginning work on the project?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Baseline Machine Learning Flow\n",
    "\n",
    "A basic baseline in a machine learning model is the simplest possible model that can be used to make predictions on the dataset. The basic baseline can be as simple as predicting the most frequent class for a classification problem or the mean value of the target variable for a regression problem. The purpose of establishing a baseline is to provide a benchmark for evaluating the performance of more complex models. A model that cannot outperform the basic baseline is considered to be useless and should not be used in practice.\n",
    "\n",
    "Here you will need to convert the code from above that was used to perform preprocessing and EDA on the dataset and create a Flow to run in order to train a baseline model. \n",
    "\n",
    "**NOTE:** It is important to realise that this is being run as a separate file and therefore re-using functions from above will not work. \n",
    "\n",
    "Suggestion: Spend 2-4 hours on this section. Rememeber that the more organized your earlier work is, the easier it is to write flows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile baseline_flow.py \n",
    "from metaflow import FlowSpec, step, Flow, current, Parameter, IncludeFile, card, current\n",
    "from metaflow.cards import Table, Markdown, Artifact\n",
    "\n",
    "# TODO move your labeling function from earlier in the notebook here\n",
    "labeling_function = lambda row: 0\n",
    "\n",
    "class BaselineNLPFlow(FlowSpec):\n",
    "\n",
    "    # We can define input parameters to a Flow using Parameters\n",
    "    # More info can be found here https://docs.metaflow.org/metaflow/basics#how-to-define-parameters-for-flows\n",
    "    split_size = Parameter('split-sz', default=0.2)\n",
    "    # In order to use a file as an input parameter for a particular Flow we can use IncludeFile\n",
    "    # More information can be found here https://docs.metaflow.org/api/flowspec#includefile\n",
    "    data = IncludeFile('data', default='../data/Womens Clothing E-Commerce Reviews.csv')\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        # Step-level dependencies are loaded within a Step, instead of loading them \n",
    "        # from the top of the file. This helps us isolate dependencies in a tight scope.\n",
    "        import pandas as pd\n",
    "        import io \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # load dataset packaged with the flow.\n",
    "        # this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "        df = pd.read_csv(io.StringIO(self.data))\n",
    "\n",
    "        # filter down to reviews and labels \n",
    "        df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "        df['review_text'] = df['review_text'].astype('str')\n",
    "        _has_review_df = df[df['review_text'] != 'nan']\n",
    "        reviews = _has_review_df['review_text']\n",
    "        labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "        # Storing the Dataframe as an instance variable of the class\n",
    "        # allows us to share it across all Steps\n",
    "        # self.df is referred to as a Data Artifact now\n",
    "        # You can read more about it here https://docs.metaflow.org/metaflow/basics#artifacts\n",
    "        self.df = pd.DataFrame({'label': labels, **_has_review_df})\n",
    "        del df\n",
    "        del _has_review_df\n",
    "\n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        _df = pd.DataFrame({'review': reviews, 'label': labels})\n",
    "        self.traindf, self.valdf = train_test_split(_df, test_size=self.split_size)\n",
    "        print(f'num of rows in train set: {self.traindf.shape[0]}')\n",
    "        print(f'num of rows in validation set: {self.valdf.shape[0]}')\n",
    "\n",
    "        self.next(self.baseline)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "        \n",
    "        ### TODO: Fit and score a baseline model on the data, log the acc and rocauc as artifacts.\n",
    "        self.base_acc = 0.0\n",
    "        self.base_rocauc = 0.0\n",
    "\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @card(type='corise') # TODO: after you get the flow working, chain link on the left side nav to open your card!\n",
    "    @step\n",
    "    def end(self):\n",
    "\n",
    "        msg = 'Baseline Accuracy: {}\\nBaseline AUC: {}'\n",
    "        print(msg.format(\n",
    "            round(self.base_acc,3), round(self.base_rocauc,3)\n",
    "        ))\n",
    "\n",
    "        current.card.append(Markdown(\"# Womens Clothing Review Results\"))\n",
    "        current.card.append(Markdown(\"## Overall Accuracy\"))\n",
    "        current.card.append(Artifact(self.base_acc))\n",
    "\n",
    "        current.card.append(Markdown(\"## Examples of False Positives\"))\n",
    "        # TODO: compute the false positive predictions where the baseline is 1 and the valdf label is 0. \n",
    "        # TODO: display the false_positives dataframe using metaflow.cards\n",
    "        # Documentation: https://docs.metaflow.org/api/cards#table\n",
    "        \n",
    "        current.card.append(Markdown(\"## Examples of False Negatives\"))\n",
    "        # TODO: compute the false positive predictions where the baseline is 0 and the valdf label is 1. \n",
    "        # TODO: display the false_negatives dataframe using metaflow.cards\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BaselineNLPFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python baseline_flow.py run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job completing Task 3!\n",
    "\n",
    "The project for Week 1 is completed but you are free to try out Task 4 below if you have the time to do so! Remember that completing Task 4 is not a requirement and completely optional. So far we have got you already building basic Machine Learning Pipelines uing Metaflow, what do you think about it so far?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: A Good First Machine Learning Model Flow(OPTIONAL)\n",
    "### Great now that we have established a baseline score for our dataset, let's try to create an actual model for this. \n",
    "\n",
    "You are free to experiment and try to get a very high score in terms of model metrics such as Accuracy/AUC but bear in-mind that usually in the real-world we have to prioritise quick iterations in order to gather quick feedback. Its important to also think about how you will be serving the model as well, more complex DNN will require GPU's in order to perform inference whereas simpler models will work fine on CPU's. \n",
    "\n",
    "1. What model do you think would do a good enough job to get the ball rolling?\n",
    "2. Try creating a `GoodFirstModelNLPFlow` Flow. \n",
    "3. Can you verify that the `GoodFirstModelNLPFlow` is indeed better than the `BaselineNLPFlow`? Metaflow has a Client API which you can use to assert that this more complex model is indeed better. You can refer to the documentation [here](https://docs.metaflow.org/metaflow/client). \n",
    "4. Did your `GoodFirstModelNLPFlow` do better than the `BaselineNLPFlow`? If yes, why do you think so? If not, what do you think was the reason?\n",
    "5. Why do you think it is important to prioritise quick iterations when working on an ML project?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-stack-metaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
